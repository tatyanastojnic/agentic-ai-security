# agentic-ai-security
Agentic AI cybersecurity guidelines and tools

This is a page dedicated to sharing agentic AI security tips and tools.

## Common Cyber attacks

### Prompt Injection
Prompt injection is an attack where malicious actors manipulate Large Language Models (LLMs) by crafting input prompts to make the AI ignore its original instructions and follow the attacker's unauthorized commands instead. In agentic AI, where LLMs act autonomously, prompt injection can lead to serious consequences such as sensitive data exposure, system malfunction, or even account takeover and remote code execution. Attackers achieve this by disguising malicious commands as natural language input, effectively "socially engineering" the AI to reveal information, perform unwanted actions, or bypass safety features. 

### Data Exfiltration 

### Sensitive Information leakage

## Benchmarking

When evaluating the security of an agentic AI solution; there are several factors which must be considered both related to Security Metrics and Quality/Scope.

Some benchmarking factors include:



## Security Testing scripts
In tools there is security tests function which contains a library of possible tools for testing vulnerability to prompt injection attacks

### LLM Security Testing Tool
A unit testing tool which can be used to test an LLM against a series of different cyber attacks including prompt injection and exfiltration. 

## Checklists


## Useful Resources
AI-DEFENSE Framework: https://github.com/edward-playground/aidefense-framework?tab=readme-ov-file A useful knowledge-base of AI threats mapped to known threats from established frameworks like MITRE ATLAS, MAESTRO, and the OWASP Top 10 for LLMs and ML.

OWASP Agentic AI Threats and Mitigations: https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/ 

Securing Agentic AI: Strategies to Prevent Unauthorized Access and Misuse https://akitra.com/securing-agentic-ai/ 